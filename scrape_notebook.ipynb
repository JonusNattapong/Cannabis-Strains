{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465bf409",
   "metadata": {},
   "source": [
    "# Cannabis Strains Web Scraper\n",
    "Scrape cannabis strain data from Seed City website.\n",
    "\n",
    "This notebook replicates the functionality of `scrape_seed_city.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b3b16",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Set\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup, Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a643e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.seed-city.com/en/list-all-products\"\n",
    "SITE_ROOT = \"https://www.seed-city.com\"\n",
    "PAGE_SIZE = 30\n",
    "REQUEST_PAUSE_SEC = 0.6  # be polite to the remote server\n",
    "MAX_EMPTY_PAGES = 3\n",
    "OUTPUT_PATH = Path(\"cannabis-strains.csv\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce3faf",
   "metadata": {},
   "source": [
    "## Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683aff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StrainRecord:\n",
    "    strain_name: str\n",
    "    breeder: str\n",
    "    description: str\n",
    "    current_price_gbp: Optional[float]\n",
    "    original_price_gbp: Optional[float]\n",
    "    discount_percent: Optional[float]\n",
    "    pack_options: str\n",
    "    product_url: str\n",
    "    image_url: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa88e4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_price(value: Optional[str]) -> Optional[float]:\n",
    "    if not value:\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+(?:\\.\\d+)?)\", value.replace(\",\", \"\"))\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "def parse_discount(value: Optional[str]) -> Optional[float]:\n",
    "    if not value:\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+(?:\\.\\d+)?)\", value)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "def extract_text(tag: Optional[Tag]) -> str:\n",
    "    return tag.get_text(\" \", strip=True) if tag else \"\"\n",
    "\n",
    "\n",
    "def clean_pack_option(option_text: str) -> str:\n",
    "    label = option_text.split(\"(\", 1)[0].strip()\n",
    "    numbers = re.findall(r\"(\\d+(?:\\.\\d+)?)\", option_text.replace(\",\", \"\"))\n",
    "    if numbers:\n",
    "        price = numbers[-1]\n",
    "        return f\"{label} (GBP {price})\"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c7bb6",
   "metadata": {},
   "source": [
    "## Parse Product Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_item(item: Tag) -> StrainRecord:\n",
    "    thumb = item.select_one(\".yagendoo_vm_browse_thumb\")\n",
    "    title_attr = thumb.get(\"title\") if thumb else \"\"\n",
    "    title_text = title_attr.strip() if title_attr else extract_text(item.select_one(\".yagendoo_vm_browse_product_title\"))\n",
    "\n",
    "    strain_name = title_text\n",
    "    breeder = \"\"\n",
    "    if \" - \" in title_text:\n",
    "        strain_name, breeder = [part.strip() for part in title_text.rsplit(\" - \", 1)]\n",
    "\n",
    "    description = extract_text(item.select_one(\".yagendoo_vm_browse_s_desc\"))\n",
    "\n",
    "    current_price_text = extract_text(item.select_one(\".yagendoo_productPrice\"))\n",
    "    original_price_text = extract_text(item.select_one(\".yagendoo_productOldPrice\"))\n",
    "    discount_text = extract_text(item.select_one(\".yagendoo_productOldPrice_box span.yagendoo_DiscountAmount\"))\n",
    "\n",
    "    pack_select = item.select_one(\"select\")\n",
    "    pack_options = []\n",
    "    if pack_select:\n",
    "        raw_options = [extract_text(option) for option in pack_select.select(\"option\")]\n",
    "        pack_options = [clean_pack_option(text) for text in raw_options if text]\n",
    "\n",
    "    image = item.select_one(\".yagendoo_vm_browse_thumb img\")\n",
    "    image_url_raw = \"\"\n",
    "    if image:\n",
    "        image_url_raw = image.get(\"data-src\") or image.get(\"src\") or \"\"\n",
    "    image_url = urljoin(SITE_ROOT, image_url_raw)\n",
    "\n",
    "    product_href = thumb.get(\"href\") if thumb else \"\"\n",
    "    product_url = urljoin(SITE_ROOT, product_href)\n",
    "\n",
    "    return StrainRecord(\n",
    "        strain_name=strain_name,\n",
    "        breeder=breeder,\n",
    "        description=description,\n",
    "        current_price_gbp=parse_price(current_price_text),\n",
    "        original_price_gbp=parse_price(original_price_text),\n",
    "        discount_percent=parse_discount(discount_text),\n",
    "        pack_options=\" | \".join(pack_options),\n",
    "        product_url=product_url,\n",
    "        image_url=image_url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57d0e0",
   "metadata": {},
   "source": [
    "## Fetch Page Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(scraper: cloudscraper.CloudScraper, offset: int) -> Optional[str]:\n",
    "    params = {\"limit\": PAGE_SIZE, \"limitstart\": offset}\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            response = scraper.get(BASE_URL, params=params, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            logging.warning(\"Non-200 status (%s) for offset %s\", response.status_code, offset)\n",
    "        except Exception as exc:\n",
    "            logging.warning(\"Request error for offset %s (attempt %s/5): %s\", offset, attempt + 1, exc)\n",
    "        time.sleep(1 + attempt)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d591cc0",
   "metadata": {},
   "source": [
    "## Collect Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_records() -> List[StrainRecord]:\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    records: List[StrainRecord] = []\n",
    "    seen_urls: Set[str] = set()\n",
    "    empty_pages = 0\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        logging.info(\"Fetching products %s - %s\", offset + 1, offset + PAGE_SIZE)\n",
    "        html = fetch_page(scraper, offset)\n",
    "        if not html:\n",
    "            empty_pages += 1\n",
    "            if empty_pages >= MAX_EMPTY_PAGES:\n",
    "                logging.info(\"Stopping after %s consecutive empty pages.\", empty_pages)\n",
    "                break\n",
    "            offset += PAGE_SIZE\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        items = soup.select(\"div.yagendoo_vm_browse_element\")\n",
    "        if not items:\n",
    "            empty_pages += 1\n",
    "            logging.info(\"No items found on page starting at %s.\", offset)\n",
    "            if empty_pages >= MAX_EMPTY_PAGES:\n",
    "                logging.info(\"Reached maximum consecutive empty pages. Ending crawl.\")\n",
    "                break\n",
    "            offset += PAGE_SIZE\n",
    "            time.sleep(REQUEST_PAUSE_SEC)\n",
    "            continue\n",
    "\n",
    "        empty_pages = 0\n",
    "\n",
    "        for item in items:\n",
    "            record = parse_item(item)\n",
    "            if record.product_url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(record.product_url)\n",
    "            records.append(record)\n",
    "\n",
    "        offset += PAGE_SIZE\n",
    "        time.sleep(REQUEST_PAUSE_SEC)\n",
    "\n",
    "        if offset > 10000:\n",
    "            logging.info(\"Reached offset safeguard (10000). Ending crawl.\")\n",
    "            break\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db85f4b",
   "metadata": {},
   "source": [
    "## Write CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e396dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(records: List[StrainRecord], path: Path) -> None:\n",
    "    if not records:\n",
    "        logging.warning(\"No records to write.\")\n",
    "        return\n",
    "\n",
    "    fieldnames = list(asdict(records[0]).keys())\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for record in records:\n",
    "            writer.writerow(asdict(record))\n",
    "\n",
    "    logging.info(\"Wrote %s records to %s\", len(records), path.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff606c",
   "metadata": {},
   "source": [
    "## Run Scraper\n",
    "Execute the scraping process and save to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e53a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scraper\n",
    "records = collect_records()\n",
    "write_csv(records, OUTPUT_PATH)\n",
    "\n",
    "print(f\"\\nScraping complete! Total records collected: {len(records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0351113",
   "metadata": {},
   "source": [
    "## Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(OUTPUT_PATH)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
